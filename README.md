# ğŸ§  ARdL â€” Minimal Deep Learning Library (NumPy Only)

**ARdL**, saf **NumPy** kullanÄ±larak sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸ, kÃ¼Ã§Ã¼k veri setleri Ã¼zerinde derin Ã¶ÄŸrenme (Deep Learning) algoritmalarÄ±nÄ± anlamak ve denemek iÃ§in oluÅŸturulmuÅŸ Ã¶zel bir projedir.  
Bu kÃ¼tÃ¼phane, hem **Yapay Sinir AÄŸlarÄ±nÄ± (NN / MLP)** hem de **KonvolÃ¼syonel Sinir AÄŸlarÄ±nÄ± (CNN)** destekler.

---

## ğŸš€ Ã–zellikler | Features

- ğŸ’¡ Pure NumPy implementation â€” no TensorFlow / PyTorch
- ğŸ§© Two models included:
  - Feedforward Neural Network (MLP)
  - Convolutional Neural Network (CNN)
- ğŸ” Custom forward & backward propagation
- âš™ï¸ Adjustable hyperparameters (learning rate, epochs)
- ğŸ§  Educational â€” designed for learning, debugging and visualization

---
## ğŸ—‚ï¸ Proje YapÄ±sÄ± | Project Structure

ARdL/
â”œâ”€â”€ ARdL_lib/
â”‚   â”œâ”€â”€ ardl/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train_cnn.py      # Convolutional Neural Network implementation
â”‚   â”‚   â””â”€â”€ train_nn.py       # Fully Connected Neural Network (MLP)
â”œâ”€â”€ main.py                   # Example training script (CNN by default)
â”œâ”€â”€ xyz.npz                    # Dataset file (NumPy format)
â””â”€â”€ README.md
## MLP Ä°ris Dataset SonuÃ§larÄ±












## MLP NasÄ±l Ã‡alÄ±ÅŸÄ±r? | How the NN Works
### Forward Propagation:

**Tr:**
Girdi verisini alÄ±r ve rastgele aÄŸÄ±rlÄ±klara sahip bir yapay sinir aÄŸÄ±ndan geÃ§irir, aktivasyon fonksiyonlarÄ± uygulanÄ±r.
Bu sÄ±rada loss (kayÄ±p) fonksiyonlarÄ± ile hatalar hesaplanÄ±r.
Bu projede kayÄ±p fonksiyonu sadece CategoricalCrossEntropyâ€™dir.

**En:**
The input data is passed once through a neural network with randomly initialized weights.
Activation functions are applied at each layer.
Meanwhile, a loss function calculates the model's error.
In this project, the only loss function used is Categorical Cross Entropy.

### Back Propagation:

**Tr:**
Hesaplanan kayÄ±plar (hatalar) ile learning rate Ã§arpÄ±lÄ±r, bunlarÄ±n tÃ¼revleri (gradyanlarÄ±) alÄ±nÄ±r.
Yapay sinir aÄŸÄ±ndaki her bir nÃ¶ronun aÄŸÄ±rlÄ±k ve bias sayÄ±larÄ±, bu tÃ¼revler Ã¼zerinden yeniden gÃ¼ncellenir.

**En:**
The calculated losses are multiplied by the learning rate, and the gradients (derivatives) are computed.
Based on these gradients, the neuronsâ€™ weights and biases are updated.

### Epochs:

**Tr:**
Forward Propagation ve Back Propagation, her dÃ¶ngÃ¼de veriye gÃ¶re tekrar hesaplanÄ±r.
Hesaplanan deÄŸerler ile nÃ¶ronlarÄ±n aÄŸÄ±rlÄ±klarÄ± ve bias sayÄ±larÄ±, her adÄ±mda biraz daha istenen seviyeye gelir.

**En:**
During each epoch, the Forward and Back Propagation steps are repeated.
With each iteration, the weights and biases are adjusted closer to the desired values.

### Optimization

**Tr:**
Modelin Ã¶ÄŸrenmesini hÄ±zlandÄ±rmak ve daha doÄŸru hale getirmek iÃ§in optimizasyon algoritmalarÄ± kullanÄ±lÄ±r. 
En yaygÄ±n kullanÄ±lan algoritmalar arasÄ±nda SGD (Stochastic Gradient Descent), Adam, RMSProp gibi yÃ¶ntemler vardÄ±r.
Bu algoritmalar, aÄŸÄ±rlÄ±k gÃ¼ncellemelerini daha verimli ve istikrarlÄ± bir ÅŸekilde yapmamÄ±zÄ± saÄŸlar.

Åu anda bu projede kullanÄ±lan tek optimizasyon algoritmasÄ± SGD (Stochastic Gradient Descent) dir. 

**En:**
Optimization algorithms are used to accelerate learning and improve model accuracy.
Common algorithms include SGD (Stochastic Gradient Descent), Adam, RMSProp, among others.
These methods make the weight updates more efficient and stable, helping the network converge faster and achieve 
better performance.

Currently the only optimization algorithm used in this project is SGD (Stochastic Gradient Descent).

## CNN NasÄ±l Ã‡alÄ±ÅŸÄ±r | How the CNN works?

### Convolution 
**Tr:**
Girdi verisini, Ã¶rnek olarak bir resmi alÄ±r. ArdÄ±ndan, Ã¶nceden tanÄ±mlanmÄ±ÅŸ Kernel (filtre) adÄ± verilen kÃ¼Ã§Ã¼k matrislerle
gÃ¶rseldeki pikseller Ã§arpÄ±lÄ±r. Bu iÅŸlem, gÃ¶rselden Ã¶zellik Ã§Ä±karmak iÃ§in yapÄ±lÄ±r. Kerneller piksellerle Ã§arpÄ±ldÄ±ÄŸÄ±nda,
gÃ¶rseldeki kenarlar, ÅŸekiller, renk geÃ§iÅŸleri gibi resme Ã¶zel olan Ã¶zellikler, resimde daha baskÄ±n olarak gÃ¶sterilir.

Kernel, bÃ¼tÃ¼n resmin pikselleri ile teker teker Ã§arpÄ±ldÄ±ÄŸÄ±nda, ortaya yeni bir resim Ã§Ä±kar. Buna feature map denir.
Feature map (Ã¶zellik haritasÄ±), ana resimdeki kenar, ÅŸekil gibi Ã¶zniteliklerin daha belirgin olarak gÃ¶zÃ¼ktÃ¼ÄŸÃ¼
yeni bir resimdir.

**En:**
The input data, for example an image, is taken and multiplied with small matrices called kernels (filters),
which are predefined. This operation is performed to extract features from the image.
When the kernels are multiplied with the pixels, the unique characteristics of the image â€” such as edges, shapes,
and color transitions â€” become more prominent in the resulting representation.

When the kernel is applied across all the pixels of the image, a new image is produced, called a feature map.
The feature map is a new version of the image where attributes like edges and shapes appear more clearly.


### ReLu (Rectified Linear Unit)
**Tr:**
Feature map Ã¼zerinde sÄ±fÄ±rÄ±n altÄ±nda deÄŸerler gÃ¶steren yerler de Ã§Ä±kacaktÄ±r ancak bu deÄŸerler genelde modeller iÃ§in 
Ã¶nemsizdir bu sebeple feature map a relu aktivasyonunu uygularÄ±z bu sÄ±fÄ±rÄ±n altÄ±ndaki deÄŸerleri sÄ±fÄ±ra sabitler
ve sÄ±fÄ±rÄ±n Ã¼zerindeki deÄŸerleri de aynen kendisine geri getirir elimizde artÄ±k yeni bir feature map var 
sÄ±fÄ±rÄ±n altÄ±nda deÄŸeri olmayan bu feature map artÄ±k poolinge girmee hazÄ±rdÄ±r 

**En:**
Some values in the feature map may be below zero, but these negative values are usually not important for the model.
Therefore, we apply the ReLU activation to the feature map. ReLU sets all negative values to zero while keeping positive values unchanged. 
The resulting feature map, now without negative values, is ready to be passed to the pooling layer.


### Pooling
**Tr:**
Elde ettiÄŸimiz feature map bize Ã¶zellikleri detaylÄ±ca gÃ¶sterir ancak hem bu kadar detaylÄ± Ã¶zelliklere ihtiyaÃ§ olmaz 
hemde gÃ¶rselde gereksiz Ã¶zellikler de barÄ±nÄ±r
YÃ¼ksek detaylÄ± gÃ¶rÃ¼ntÃ¼leri iÅŸlemek hem iÅŸlem gÃ¼cÃ¼nÃ¼ boÅŸa harcamak hemde eÄŸitim sÃ¼resini aÅŸÄ±rÄ± uzatmaya sebep olur 
bu yÃ¼zden yeni bir gÃ¶rsele ihtiyacÄ±mÄ±z olur daha kÃ¼Ã§Ã¼k ama aradÄ±ÄŸÄ±mÄ±z Ã¶zellikleri de iÃ§inde barÄ±ndÄ±ran bir gÃ¶rsel
bu gÃ¶rseli elde edebilmek iÃ§in pooling kullanÄ±rÄ±z feature map e GlobalAvgPool veya MaxPooling gibi pooling algoritmalarÄ±na
ihtiyaÃ§ duyarÄ±z Pooling algoritmalarÄ± ile gÃ¶rÃ¼ntÃ¼deki gereksiz Ã¶zellikleri ve Ã§ok detaylÄ± olan bu Ã¶zellikleri filtrelemiÅŸ oluruz
bu yÃ¶ntem ile hem yapay sinir aÄŸÄ±mÄ±z Ã¶ÄŸrenmesi gereken veriye daha Ã§ok odaklanabilir hemde verinin boyutu azaldÄ±ÄŸÄ± iÃ§in 
iÅŸ yÃ¼kÃ¼mÃ¼z azalÄ±r 
Bu Projede kullanÄ±lan tek pooling algoritmasÄ± MaxPooling dir 

**En:**
The obtained feature map shows the image features in detail; however, such a high level of detail is often unnecessary,
and redundant information may also exist in the image.
Processing high-resolution, detailed data wastes computational power and significantly increases training time.
Therefore, we need a new image representation that is smaller but still contains the essential features we want to preserve.

To achieve this, we use pooling. Pooling algorithms such as Global Average Pooling or MaxPooling
are applied to the feature map. These algorithms filter out redundant or overly detailed features in the image.

With this method, the neural network can focus more on the important data it needs to learn,
and since the data size is reduced, the computational load decreases as well.

The only pooling algorithm used in this Project is MaxPooling.


### Classification
**Tr:**
Pooling iÅŸleminden sonra verimiz bir MLP aÄŸÄ±na girer ve sÄ±nÄ±flandÄ±rÄ±lÄ±r. Ä°kili sÄ±nÄ±flandÄ±rmalarda genellikle Sigmoid,
Ã§oklu sÄ±nÄ±flandÄ±rmalarda ise Softmax fonksiyonu kullanÄ±lÄ±r.

**En:**
After pooling, the data is passed to an MLP network for classification. In binary classification, 
Sigmoid is generally used, while in multi-class classification, Softmax is applied.


### ğŸ–Šï¸ Author | Yazar

**Ali Arhan Ä°la**  
[GitHub Profile](https://github.com/aliarhanila)  
[LinkedIn Profile](https://www.linkedin.com/in/ali-arhan-ila-693a2830b/)
