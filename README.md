# ğŸ§  ARdL â€” Minimal Deep Learning Library (NumPy Only)

**ARdL**, saf **NumPy** kullanÄ±larak sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸ, kÃ¼Ã§Ã¼k veri setleri Ã¼zerinde derin Ã¶ÄŸrenme (Deep Learning) algoritmalarÄ±nÄ± anlamak ve denemek iÃ§in oluÅŸturulmuÅŸ Ã¶zel bir projedir.  
Bu kÃ¼tÃ¼phane, hem **Yapay Sinir AÄŸlarÄ±nÄ± (NN / MLP)** hem de **KonvolÃ¼syonel Sinir AÄŸlarÄ±nÄ± (CNN)** destekler.

---

## ğŸš€ Ã–zellikler | Features

- ğŸ’¡ Pure NumPy implementation â€” no TensorFlow / PyTorch
- ğŸ§© Two models included:
  - Feedforward Neural Network (MLP)
  - Convolutional Neural Network (CNN)
- ğŸ” Custom forward & backward propagation
- âš™ï¸ Adjustable hyperparameters (learning rate, epochs)
- ğŸ§  Educational â€” designed for learning, debugging and visualization
- ğŸ’¾ Model Save - Load Model

---
## MLP Ä°ris Dataset SonuÃ§larÄ±
![MLP Training Results on Iris Dataset](assets/iris_results.png)


MLP modeli Iris veri setindeki Ã¶rÃ¼ntÃ¼leri baÅŸarÄ±yla Ã¶ÄŸrenmiÅŸtir 
ve test kÃ¼mesinde **%96â€™nÄ±n Ã¼zerinde doÄŸruluk** elde etmiÅŸtir.  
Bu, tam baÄŸlÄ± basit bir yapay sinir aÄŸÄ±nÄ±n bile 
yapÄ±landÄ±rÄ±lmÄ±ÅŸ veriler Ã¼zerinde etkili sonuÃ§lar verebileceÄŸini gÃ¶sterir.


## Model Kaydetmek ve YÃ¼klemek 
### Save
np.savez("mlp_model.npz", weights=W_dense, biases=b_dense)

### Load
data = np.load("mlp_model.npz", allow_pickle=True)
W_dense_loaded = data['weights']
b_dense_loaded = data['biases']

---
## MLP NasÄ±l Ã‡alÄ±ÅŸÄ±r? | How the NN Works
### Forward Propagation:

**Tr:**
Girdi verisini alÄ±r ve rastgele aÄŸÄ±rlÄ±klara sahip bir yapay sinir aÄŸÄ±ndan geÃ§irir, aktivasyon fonksiyonlarÄ± uygulanÄ±r.
Bu sÄ±rada loss (kayÄ±p) fonksiyonlarÄ± ile hatalar hesaplanÄ±r.
Bu projede kayÄ±p fonksiyonu sadece CategoricalCrossEntropyâ€™dir.

**En:**
The input data is passed once through a neural network with randomly initialized weights.
Activation functions are applied at each layer.
Meanwhile, a loss function calculates the model's error.
In this project, the only loss function used is Categorical Cross Entropy.

### Back Propagation:

**Tr:**
Hesaplanan kayÄ±plar (hatalar) ile learning rate Ã§arpÄ±lÄ±r, bunlarÄ±n tÃ¼revleri (gradyanlarÄ±) alÄ±nÄ±r.
Yapay sinir aÄŸÄ±ndaki her bir nÃ¶ronun aÄŸÄ±rlÄ±k ve bias sayÄ±larÄ±, bu tÃ¼revler Ã¼zerinden yeniden gÃ¼ncellenir.

**En:**
The calculated losses are multiplied by the learning rate, and the gradients (derivatives) are computed.
Based on these gradients, the neuronsâ€™ weights and biases are updated.

### Epochs:

**Tr:**
Forward Propagation ve Back Propagation, her dÃ¶ngÃ¼de veriye gÃ¶re tekrar hesaplanÄ±r.
Hesaplanan deÄŸerler ile nÃ¶ronlarÄ±n aÄŸÄ±rlÄ±klarÄ± ve bias sayÄ±larÄ±, her adÄ±mda biraz daha istenen seviyeye gelir.

**En:**
During each epoch, the Forward and Back Propagation steps are repeated.
With each iteration, the weights and biases are adjusted closer to the desired values.

### Optimization

**Tr:**
Modelin Ã¶ÄŸrenmesini hÄ±zlandÄ±rmak ve daha doÄŸru hale getirmek iÃ§in optimizasyon algoritmalarÄ± kullanÄ±lÄ±r. 
En yaygÄ±n kullanÄ±lan algoritmalar arasÄ±nda SGD (Stochastic Gradient Descent), Adam, RMSProp gibi yÃ¶ntemler vardÄ±r.
Bu algoritmalar, aÄŸÄ±rlÄ±k gÃ¼ncellemelerini daha verimli ve istikrarlÄ± bir ÅŸekilde yapmamÄ±zÄ± saÄŸlar.

Åu anda bu projede kullanÄ±lan tek optimizasyon algoritmasÄ± SGD (Stochastic Gradient Descent) dir. 

**En:**
Optimization algorithms are used to accelerate learning and improve model accuracy.
Common algorithms include SGD (Stochastic Gradient Descent), Adam, RMSProp, among others.
These methods make the weight updates more efficient and stable, helping the network converge faster and achieve 
better performance.

Currently the only optimization algorithm used in this project is SGD (Stochastic Gradient Descent).

---
# CNN Mnist Dataset SonuÃ§larÄ± 
![CNN Training Results on Mnist Dataset](assets/mnist_results.png)

20 epochda 0.001 lr deÄŸeri ile elde edilen sonuÃ§lar
Epoch 20: Train Loss=0.2815, Train Acc=92.80% | Test Loss=0.1832, Test Acc=94.00%

---
## CNN NasÄ±l Ã‡alÄ±ÅŸÄ±r | How the CNN works?

### Convolution 
**Tr:**
Girdi verisini, Ã¶rnek olarak bir resmi alÄ±r. ArdÄ±ndan, Ã¶nceden tanÄ±mlanmÄ±ÅŸ Kernel (filtre) adÄ± verilen kÃ¼Ã§Ã¼k matrislerle
gÃ¶rseldeki pikseller Ã§arpÄ±lÄ±r. Bu iÅŸlem, gÃ¶rselden Ã¶zellik Ã§Ä±karmak iÃ§in yapÄ±lÄ±r. Kerneller piksellerle Ã§arpÄ±ldÄ±ÄŸÄ±nda,
gÃ¶rseldeki kenarlar, ÅŸekiller, renk geÃ§iÅŸleri gibi resme Ã¶zel olan Ã¶zellikler, resimde daha baskÄ±n olarak gÃ¶sterilir.

Kernel, bÃ¼tÃ¼n resmin pikselleri ile teker teker Ã§arpÄ±ldÄ±ÄŸÄ±nda, ortaya yeni bir resim Ã§Ä±kar. Buna feature map denir.
Feature map (Ã¶zellik haritasÄ±), ana resimdeki kenar, ÅŸekil gibi Ã¶zniteliklerin daha belirgin olarak gÃ¶zÃ¼ktÃ¼ÄŸÃ¼
yeni bir resimdir.

**En:**
The input data, for example an image, is taken and multiplied with small matrices called kernels (filters),
which are predefined. This operation is performed to extract features from the image.
When the kernels are multiplied with the pixels, the unique characteristics of the image â€” such as edges, shapes,
and color transitions â€” become more prominent in the resulting representation.

When the kernel is applied across all the pixels of the image, a new image is produced, called a feature map.
The feature map is a new version of the image where attributes like edges and shapes appear more clearly.


### ReLu (Rectified Linear Unit)
**Tr:**
Feature map Ã¼zerinde sÄ±fÄ±rÄ±n altÄ±nda deÄŸerler gÃ¶steren yerler de Ã§Ä±kacaktÄ±r ancak bu deÄŸerler genelde modeller iÃ§in 
Ã¶nemsizdir bu sebeple feature map a relu aktivasyonunu uygularÄ±z bu sÄ±fÄ±rÄ±n altÄ±ndaki deÄŸerleri sÄ±fÄ±ra sabitler
ve sÄ±fÄ±rÄ±n Ã¼zerindeki deÄŸerleri de aynen kendisine geri getirir elimizde artÄ±k yeni bir feature map var 
sÄ±fÄ±rÄ±n altÄ±nda deÄŸeri olmayan bu feature map artÄ±k poolinge girmee hazÄ±rdÄ±r 

**En:**
Some values in the feature map may be below zero, but these negative values are usually not important for the model.
Therefore, we apply the ReLU activation to the feature map. ReLU sets all negative values to zero while keeping positive values unchanged. 
The resulting feature map, now without negative values, is ready to be passed to the pooling layer.


### Pooling
**Tr:**
Elde ettiÄŸimiz feature map bize Ã¶zellikleri detaylÄ±ca gÃ¶sterir ancak hem bu kadar detaylÄ± Ã¶zelliklere ihtiyaÃ§ olmaz 
hemde gÃ¶rselde gereksiz Ã¶zellikler de barÄ±nÄ±r
YÃ¼ksek detaylÄ± gÃ¶rÃ¼ntÃ¼leri iÅŸlemek hem iÅŸlem gÃ¼cÃ¼nÃ¼ boÅŸa harcamak hemde eÄŸitim sÃ¼resini aÅŸÄ±rÄ± uzatmaya sebep olur 
bu yÃ¼zden yeni bir gÃ¶rsele ihtiyacÄ±mÄ±z olur daha kÃ¼Ã§Ã¼k ama aradÄ±ÄŸÄ±mÄ±z Ã¶zellikleri de iÃ§inde barÄ±ndÄ±ran bir gÃ¶rsel
bu gÃ¶rseli elde edebilmek iÃ§in pooling kullanÄ±rÄ±z feature map e GlobalAvgPool veya MaxPooling gibi pooling algoritmalarÄ±na
ihtiyaÃ§ duyarÄ±z Pooling algoritmalarÄ± ile gÃ¶rÃ¼ntÃ¼deki gereksiz Ã¶zellikleri ve Ã§ok detaylÄ± olan bu Ã¶zellikleri filtrelemiÅŸ oluruz
bu yÃ¶ntem ile hem yapay sinir aÄŸÄ±mÄ±z Ã¶ÄŸrenmesi gereken veriye daha Ã§ok odaklanabilir hemde verinin boyutu azaldÄ±ÄŸÄ± iÃ§in 
iÅŸ yÃ¼kÃ¼mÃ¼z azalÄ±r 
Bu Projede kullanÄ±lan tek pooling algoritmasÄ± MaxPooling dir 

**En:**
The obtained feature map shows the image features in detail; however, such a high level of detail is often unnecessary,
and redundant information may also exist in the image.
Processing high-resolution, detailed data wastes computational power and significantly increases training time.
Therefore, we need a new image representation that is smaller but still contains the essential features we want to preserve.

To achieve this, we use pooling. Pooling algorithms such as Global Average Pooling or MaxPooling
are applied to the feature map. These algorithms filter out redundant or overly detailed features in the image.

With this method, the neural network can focus more on the important data it needs to learn,
and since the data size is reduced, the computational load decreases as well.

The only pooling algorithm used in this Project is MaxPooling.


### Classification
**Tr:**
Pooling iÅŸleminden sonra verimiz bir MLP aÄŸÄ±na girer ve sÄ±nÄ±flandÄ±rÄ±lÄ±r. Ä°kili sÄ±nÄ±flandÄ±rmalarda genellikle Sigmoid,
Ã§oklu sÄ±nÄ±flandÄ±rmalarda ise Softmax fonksiyonu kullanÄ±lÄ±r.

**En:**
After pooling, the data is passed to an MLP network for classification. In binary classification, 
Sigmoid is generally used, while in multi-class classification, Softmax is applied.

---
## GÃ¶zlemlerim 
En ilginÃ§ gÃ¶zlemim **random_seed** fonksiyonunda oldu.
Bu fonksiyon genelde gÃ¶z ardÄ± edilir; ancak gerÃ§ek potansiyeli fark edildiÄŸinde hem oldukÃ§a kullanÄ±ÅŸlÄ± hem de zaman kazandÄ±rÄ±cÄ± bir araÃ§tÄ±r.

Genel olarak, MLP ve CNN algoritmalarÄ± veriyi ilk kez **rastgele katsayÄ±lar (aÄŸÄ±rlÄ±klar) ve bias deÄŸerleriyle** iÅŸler.
Sinir aÄŸÄ±, bu rastgele deÄŸerlerle bir tahmin Ã¼retir ve tahminin gerÃ§ek deÄŸerden ne kadar saptÄ±ÄŸÄ±na gÃ¶re katsayÄ±larÄ±nÄ± gÃ¼nceller.
Bu sÃ¼reÃ§ â€”tahmin, hata hesaplama, aÄŸÄ±rlÄ±k gÃ¼ncellemeâ€” defalarca tekrarlanarak modelin gerÃ§ek deÄŸerlere yaklaÅŸmasÄ± saÄŸlanÄ±r.

Bu iÅŸlemin ne kadar sÃ¼receÄŸini **learning rate (Ã¶ÄŸrenme oranÄ±)** belirler.
Learning rate ne kadar yÃ¼ksekse model o kadar hÄ±zlÄ± Ã¶ÄŸrenir;
ancak rastgele baÅŸlatÄ±lan aÄŸÄ±rlÄ±klar bÃ¼yÃ¼k deÄŸerler iÃ§eriyorsa, yÃ¼ksek learning rate modelin **kararsÄ±z hale gelmesine** yol aÃ§abilir.
Bu yÃ¼zden genellikle dÃ¼ÅŸÃ¼k bir learning rate ile daha fazla epoch kullanÄ±larak **daha stabil** bir Ã¶ÄŸrenme hedeflenir.

Fakat burada dikkat Ã§ekici bir denge vardÄ±r:
Learning rate yÃ¼kseldikÃ§e model daha hÄ±zlÄ± toparlanÄ±r, ancak stabilitesi azalÄ±r.
Ä°ÅŸte bu noktada **random_seed** devreye girer.
Random seed sayesinde, sinir aÄŸÄ±na ilk baÅŸta verilen rastgele aÄŸÄ±rlÄ±klar ve biaslar tekrarlanabilir hale gelir.
Bu da modelin her Ã§alÄ±ÅŸtÄ±rmada aynÄ± baÅŸlangÄ±Ã§tan yola Ã§Ä±kmasÄ±nÄ±, dolayÄ±sÄ±yla daha kararlÄ± ve Ã¶ngÃ¶rÃ¼lebilir sonuÃ§lar vermesini saÄŸlar.

Kendi denemelerimde, random seed eklemeden Ã¶nce aynÄ± veri, aynÄ± parametre ve aynÄ± algoritmayla bile modeli her Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mda farklÄ± sonuÃ§lar aldÄ±ÄŸÄ±mÄ± fark ettim:
Bazen hiÃ§ Ã¶ÄŸrenmiyor, bazen Ã§ok hÄ±zlÄ± Ã¶ÄŸreniyordu.
Bu farkÄ±n, modelin baÅŸlangÄ±Ã§ aÄŸÄ±rlÄ±klarÄ± ve biaslarÄ±ndan kaynaklandÄ±ÄŸÄ±nÄ± keÅŸfettim.

AÅŸaÄŸÄ±da, aynÄ± parametrelere sahip iki modelin farklÄ± baÅŸlangÄ±Ã§ aÄŸÄ±rlÄ±klarÄ± nedeniyle nasÄ±l farklÄ± sonuÃ§lar verdiÄŸini gÃ¶rebilirsiniz ğŸ‘‡

![CNN Training Results on Mnist Dataset](assets/mnist_results.png)
![CNN Training Results on Mnist Dataset](assets/mnist1_results.png)

Ä°kinci grafikte yalnÄ±zca 8 epoch var; bu yÃ¼zden ilk grafiÄŸin de **ilk 8 epochâ€™una** dikkat etmenizi Ã¶neririm.

ğŸ’¡ KÄ±sa Ã¶zet:
random_seed, kÃ¼Ã§Ã¼k veri setlerinde ve derin Ã¶ÄŸrenmede ÅŸans faktÃ¶rÃ¼nÃ¼ kontrol altÄ±na alÄ±r,
modellerin tekrarlanabilir ve daha gÃ¼venilir sonuÃ§lar vermesini saÄŸlar.


### ğŸ–Šï¸ Author | Yazar

**Ali Arhan Ä°la**  
[GitHub Profile](https://github.com/aliarhanila)  
[LinkedIn Profile](https://www.linkedin.com/in/ali-arhan-ila-693a2830b/)
